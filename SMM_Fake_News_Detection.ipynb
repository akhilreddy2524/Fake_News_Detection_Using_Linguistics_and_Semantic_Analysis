{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akhilreddy2524/Fake_News_Detection_Using_Linguistics_and_Semantic_Analysis/blob/main/SMM_Fake_News_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "82AsSHSHf66I"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import pos_tag, ne_chunk\n",
        "from nltk.chunk import conlltags2tree, tree2conlltags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iT3ImXS_aIDe"
      },
      "outputs": [],
      "source": [
        "df1 = pd.read_csv(\"/N/u/akvajral/Carbonate/Downloads/smm/Fake.csv\")\n",
        "df2 = pd.read_csv(\"/N/u/akvajral/Carbonate/Downloads/smm/True.csv\")\n",
        "#df1, df2 = df1[:500], df2[:500]\n",
        "# Add a column to indicate if the news is fake (1) or true (0)\n",
        "df1['label'] = 1\n",
        "df2['label'] = 0\n",
        "\n",
        "# Concatenate the two datasets\n",
        "df = pd.concat([df1, df2], ignore_index=True)\n",
        "# Shuffle the rows of the dataframe\n",
        "df = df.sample(frac=1).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQaFqXqelPGP",
        "outputId": "df176032-3215-48c0-c4a5-c4f2b40ab86f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     /N/u/akvajral/Carbonate/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2mkB294FloNq",
        "outputId": "8f055a8a-3345-4179-d553-c3238a87250b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /N/u/akvajral/Carbonate/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZLxIgF6l9qv",
        "outputId": "0c6c148a-4b67-4fd5-b511-5c6877a4f8d9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /N/u/akvajral/Carbonate/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AD8kWzywzyMi",
        "outputId": "27f0b665-856e-4a45-d126-c91e5dd08cd9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /N/u/akvajral/Carbonate/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('maxent_ne_chunker')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8X89VqZYz0Nj",
        "outputId": "bd45e87b-6ed8-45f9-a502-cb3a40501b84"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package words to\n",
            "[nltk_data]     /N/u/akvajral/Carbonate/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('words')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P2KG-MKKlGgc"
      },
      "outputs": [],
      "source": [
        "# Remove special characters and digits\n",
        "df_text=df['text']\n",
        "df['text'] = df['text'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
        "df['text'] = df['text'].apply(lambda x: re.sub(r'\\d+', '', x))\n",
        "\n",
        "# Tokenization\n",
        "df['text'] = df['text'].apply(word_tokenize)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lnt3PY_Plqwz"
      },
      "outputs": [],
      "source": [
        "# Remove stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "df['text'] = df['text'].apply(lambda x: [word for word in x if word.lower() not in stop_words])\n",
        "\n",
        "# Perform part-of-speech tagging\n",
        "df['pos_tags'] = df['text'].apply(lambda x: pos_tag(x))\n",
        "\n",
        "# Perform named entity recognition\n",
        "def ne_recognition(text):\n",
        "    chunked = ne_chunk(text)\n",
        "    iob_tagged = tree2conlltags(chunked)\n",
        "    return iob_tagged\n",
        "\n",
        "df['ne_tags'] = df['pos_tags'].apply(lambda x: ne_recognition(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LcbWOxfDUbcV"
      },
      "outputs": [],
      "source": [
        "# Get n-grams\n",
        "def get_ngrams(row):\n",
        "    text = row['text']\n",
        "    ngrams = list(nltk.ngrams(text, 2))\n",
        "    features = []\n",
        "    for ngram in ngrams:\n",
        "        features.append(' '.join(ngram))\n",
        "    return ' '.join(features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jqUw3gRtUfXu"
      },
      "outputs": [],
      "source": [
        "# Get named entity recognition tags\n",
        "def get_ner(row):\n",
        "    ne_tags = row['ne_tags']\n",
        "    features = []\n",
        "    for tag in ne_tags:\n",
        "        features.append(tag[1])\n",
        "    return ' '.join(features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eoSl6sIePGj3"
      },
      "outputs": [],
      "source": [
        "# Combine n-grams and named entity recognition tags\n",
        "def combine_ner_ngrams(row):\n",
        "    text = row['text']\n",
        "    ne_tags = row['ne_tags']\n",
        "    ngrams = list(nltk.ngrams(text, 2))\n",
        "    features = []\n",
        "    for tag in ne_tags:\n",
        "        features.append(tag[1])\n",
        "    for ngram in ngrams:\n",
        "        features.append(' '.join(ngram))\n",
        "    return ' '.join(features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F7jJuM4fY8Je"
      },
      "outputs": [],
      "source": [
        "# Checking with 5 true and fake news each\n",
        "def checking(fea,la,lp):\n",
        "  fea=fea.reset_index(drop=True)\n",
        "  la=la.reset_index(drop=True)\n",
        "  tr, fa, res = [], [], []\n",
        "  for i in range(len(la)):\n",
        "    if la[i]==0:\n",
        "      if len(fa)<5:\n",
        "        fa.append(i)\n",
        "    else:\n",
        "      if len(tr)<5:\n",
        "        tr.append(i)\n",
        "    if len(tr)==5 and len(fa)==5:\n",
        "      break\n",
        "  print(\"False News:\\n\")\n",
        "  for i in fa:\n",
        "    res.append(\"True\" if lp.predict(fea)[i] else \"Fake\")\n",
        "  print(res,\"\\n\\n\")\n",
        "  res = []\n",
        "  print(\"True News:\\n\")\n",
        "  for i in tr:\n",
        "    res.append(\"True\" if lp.predict(fea)[i] else \"Fake\")\n",
        "  print(res,\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EQIYPkjQkppq"
      },
      "outputs": [],
      "source": [
        " # Make predictions on validation, train, and test sets\n",
        "def pred_eval(x, y, lp):\n",
        "  y_pred = lp.predict(x)\n",
        "  print('Accuracy:', accuracy_score(y, y_pred))\n",
        "  print(classification_report(y, y_pred))\n",
        "  print(confusion_matrix(y, y_pred),\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dEaM7wNOqNwL"
      },
      "outputs": [],
      "source": [
        "def split_data(data, labels, test_size=0.2, val_size=0.2, random_state=42):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=test_size, random_state=random_state)\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=val_size/(1-test_size), random_state=random_state)\n",
        "    return X_train, X_val, X_test, y_train, y_val, y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hyU7gKpxPOGZ"
      },
      "outputs": [],
      "source": [
        "def modelling(fea,la):\n",
        "  # Split dataset into training and testing sets\n",
        "  X_train, X_test, y_train, y_test = train_test_split(fea, la, test_size=0.2, stratify=la, random_state=42)\n",
        "  X_train1, y_train1 = X_train, y_train\n",
        "  X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, stratify =y_train, random_state=42)\n",
        "\n",
        "  # Create pipeline for logistic regression model\n",
        "  lr_pipeline = Pipeline([\n",
        "      ('tfidf', TfidfVectorizer()),\n",
        "      ('clf', LogisticRegression())\n",
        "  ])\n",
        "\n",
        "  # Train the model\n",
        "  lr_pipeline.fit(X_train, y_train)\n",
        "\n",
        "  # Evaluate the model\n",
        "  print(\"For Validation:\\n\")\n",
        "  pred_eval(X_val, y_val, lr_pipeline)\n",
        "  print(\"For Training:\\n\")\n",
        "  pred_eval(X_train, y_train, lr_pipeline)\n",
        "  print(\"For Testing:\\n\")\n",
        "  pred_eval(X_test, y_test, lr_pipeline)\n",
        "\n",
        "  # Check with few known cases from test dataset\n",
        "  checking(X_test, y_test, lr_pipeline)\n",
        "  return X_train1, y_train1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XWYe2kkIPHdr",
        "outputId": "f72db1b7-8da0-4111-fdf6-c089d1e427ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "For Validation:\n",
            "\n",
            "Accuracy: 0.9898385300668151\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.99      0.99      3427\n",
            "           1       0.99      0.99      0.99      3757\n",
            "\n",
            "    accuracy                           0.99      7184\n",
            "   macro avg       0.99      0.99      0.99      7184\n",
            "weighted avg       0.99      0.99      0.99      7184\n",
            "\n",
            "[[3396   31]\n",
            " [  42 3715]] \n",
            "\n",
            "\n",
            "For Training:\n",
            "\n",
            "Accuracy: 0.9919259413934711\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.99      0.99     13706\n",
            "           1       0.99      0.99      0.99     15028\n",
            "\n",
            "    accuracy                           0.99     28734\n",
            "   macro avg       0.99      0.99      0.99     28734\n",
            "weighted avg       0.99      0.99      0.99     28734\n",
            "\n",
            "[[13612    94]\n",
            " [  138 14890]] \n",
            "\n",
            "\n",
            "For Testing:\n",
            "\n",
            "Accuracy: 0.9850779510022272\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.99      0.98      4284\n",
            "           1       0.99      0.99      0.99      4696\n",
            "\n",
            "    accuracy                           0.99      8980\n",
            "   macro avg       0.99      0.99      0.99      8980\n",
            "weighted avg       0.99      0.99      0.99      8980\n",
            "\n",
            "[[4220   64]\n",
            " [  70 4626]] \n",
            "\n",
            "\n",
            "False News:\n",
            "\n",
            "['Fake', 'Fake', 'Fake', 'Fake', 'Fake'] \n",
            "\n",
            "\n",
            "True News:\n",
            "\n",
            "['True', 'True', 'True', 'True', 'True'] \n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df['ngrams'] = df.apply(get_ngrams, axis=1)\n",
        "ngramsX, ngramsY = modelling(df['ngrams'], df['label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "terbwvOzVX2_",
        "outputId": "1accd261-83b3-4eb7-d4ab-d7597c8c6d60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "For Validation:\n",
            "\n",
            "Accuracy: 0.793847438752784\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.80      0.79      3427\n",
            "           1       0.81      0.78      0.80      3757\n",
            "\n",
            "    accuracy                           0.79      7184\n",
            "   macro avg       0.79      0.79      0.79      7184\n",
            "weighted avg       0.79      0.79      0.79      7184\n",
            "\n",
            "[[2758  669]\n",
            " [ 812 2945]] \n",
            "\n",
            "\n",
            "For Training:\n",
            "\n",
            "Accuracy: 0.7971740794877149\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.81      0.79     13706\n",
            "           1       0.82      0.79      0.80     15028\n",
            "\n",
            "    accuracy                           0.80     28734\n",
            "   macro avg       0.80      0.80      0.80     28734\n",
            "weighted avg       0.80      0.80      0.80     28734\n",
            "\n",
            "[[11074  2632]\n",
            " [ 3196 11832]] \n",
            "\n",
            "\n",
            "For Testing:\n",
            "\n",
            "Accuracy: 0.7914253897550111\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.81      0.79      4284\n",
            "           1       0.82      0.78      0.80      4696\n",
            "\n",
            "    accuracy                           0.79      8980\n",
            "   macro avg       0.79      0.79      0.79      8980\n",
            "weighted avg       0.79      0.79      0.79      8980\n",
            "\n",
            "[[3461  823]\n",
            " [1050 3646]] \n",
            "\n",
            "\n",
            "False News:\n",
            "\n",
            "['Fake', 'True', 'Fake', 'Fake', 'Fake'] \n",
            "\n",
            "\n",
            "True News:\n",
            "\n",
            "['True', 'Fake', 'True', 'True', 'True'] \n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df['ner'] = df.apply(get_ner, axis=1)\n",
        "nerX, nerY = modelling(df['ner'], df['label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lYSxyaiGVX_2",
        "outputId": "f88c981e-5de2-4e4c-fef3-28229fbf9233"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "For Validation:\n",
            "\n",
            "Accuracy: 0.9887249443207127\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.99      0.99      3427\n",
            "           1       0.99      0.99      0.99      3757\n",
            "\n",
            "    accuracy                           0.99      7184\n",
            "   macro avg       0.99      0.99      0.99      7184\n",
            "weighted avg       0.99      0.99      0.99      7184\n",
            "\n",
            "[[3391   36]\n",
            " [  45 3712]] \n",
            "\n",
            "\n",
            "For Training:\n",
            "\n",
            "Accuracy: 0.9907078722071414\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.99      0.99     13706\n",
            "           1       0.99      0.99      0.99     15028\n",
            "\n",
            "    accuracy                           0.99     28734\n",
            "   macro avg       0.99      0.99      0.99     28734\n",
            "weighted avg       0.99      0.99      0.99     28734\n",
            "\n",
            "[[13603   103]\n",
            " [  164 14864]] \n",
            "\n",
            "\n",
            "For Testing:\n",
            "\n",
            "Accuracy: 0.9828507795100223\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.98      0.98      4284\n",
            "           1       0.99      0.98      0.98      4696\n",
            "\n",
            "    accuracy                           0.98      8980\n",
            "   macro avg       0.98      0.98      0.98      8980\n",
            "weighted avg       0.98      0.98      0.98      8980\n",
            "\n",
            "[[4216   68]\n",
            " [  86 4610]] \n",
            "\n",
            "\n",
            "False News:\n",
            "\n",
            "['Fake', 'Fake', 'Fake', 'Fake', 'Fake'] \n",
            "\n",
            "\n",
            "True News:\n",
            "\n",
            "['True', 'True', 'True', 'True', 'True'] \n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df['ngrams+ner'] = df.apply(combine_ner_ngrams, axis=1)\n",
        "combX, combY = modelling(df['ngrams+ner'], df['label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mXAbjYAiYMZI"
      },
      "outputs": [],
      "source": [
        "# Although the results are impressive, it is crucial to validate the\n",
        "#performance of the model because overfitting may happen. Overfitting\n",
        "#occurs when the model becomes overly specialized to the training data,\n",
        "#and its ability to generalize to new, unseen data may be compromised.\n",
        "#To check if there is any significant decrease in accuracies, we are utilizing\n",
        "#k-fold cross-validation.\n",
        "from sklearn.model_selection import KFold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uopFCTj2z6m_"
      },
      "outputs": [],
      "source": [
        "def kfold(X,y,k):\n",
        "  kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
        "  X = np.array(X)\n",
        "  y = np.array(y)\n",
        "  scores=[]\n",
        "  for train_index, test_index in kf.split(X):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "    lr_pipeline = Pipeline([\n",
        "        ('tfidf', TfidfVectorizer()),\n",
        "        ('clf', LogisticRegression())\n",
        "    ])\n",
        "    lr_pipeline.fit(X_train,y_train)\n",
        "\n",
        "    # Testing the classifier\n",
        "    y_pred = lr_pipeline.predict(X_test)#_tfidf)\n",
        "    scores.append(accuracy_score(y_test,y_pred))\n",
        "  print(\"Accuracies:\",scores)\n",
        "  print(\"Average Accuracy:\",np.mean(scores))\n",
        "    # Printing classification report\n",
        "    #print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0tLTFfj7HPSA",
        "outputId": "96813b76-6373-4a0b-b861-9486755b7be0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Kfold for ngrams:\n",
            "Accuracies: [0.986358574610245, 0.9913697104677061, 0.9860801781737194, 0.9883073496659243, 0.986358574610245, 0.9871937639198218, 0.9894209354120267, 0.9838530066815144, 0.98635477582846, 0.9866332497911445]\n",
            "Average Accuracy: 0.9871930119160808\n"
          ]
        }
      ],
      "source": [
        "# Kfold for ngrams:\n",
        "print(\"Kfold for ngrams:\")\n",
        "k=10\n",
        "kfold(ngramsX,ngramsY,k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZMl3KNHb8_u",
        "outputId": "f0ac1176-8070-403f-a6a1-6178f416b74f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Kfold for NER:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/N/soft/rhel7/python/3.9.8/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/N/soft/rhel7/python/3.9.8/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracies: [0.794543429844098, 0.7917594654788419, 0.7900890868596881, 0.8051224944320713, 0.8023385300668151, 0.8073496659242761, 0.7937082405345212, 0.7928730512249443, 0.7956001113895851, 0.7961570593149541]\n",
            "Average Accuracy: 0.7969541135069795\n"
          ]
        }
      ],
      "source": [
        "# Kfold for NER:\n",
        "print(\"Kfold for NER:\")\n",
        "kfold(nerX,nerY,k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bb6VJy3d9RL6",
        "outputId": "f9cfbd71-2f0e-419a-fe61-72ed2274a6be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Kfold for ngrams + NER:\n",
            "Accuracies: [0.9855233853006682, 0.9891425389755011, 0.986358574610245, 0.9874721603563474, 0.984966592427617, 0.9858017817371938, 0.9888641425389755, 0.9824610244988864, 0.9855193539404066, 0.985240879977722]\n",
            "Average Accuracy: 0.9861350434363562\n"
          ]
        }
      ],
      "source": [
        "# Kfold for ngrams + NER:\n",
        "print(\"Kfold for ngrams + NER:\")\n",
        "kfold(combX,combY,k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zfIrJse3cc2W"
      },
      "outputs": [],
      "source": [
        "# After applying k-fold cross-validation, it was observed that there was\n",
        "#no significant decrease in accuracy and the average accuracy values were\n",
        "#comparable to the initial results. Therefore, it can be concluded that the\n",
        "#model did not experience overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_kz84VukHDhY",
        "outputId": "9e3270ef-0882-47ef-9659-db6dda618daf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Collecting textblob\n",
            "  Downloading textblob-0.17.1-py2.py3-none-any.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 6.4 MB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: nltk>=3.1 in /geode2/home/u010/akvajral/Carbonate/.local/lib/python3.9/site-packages (from textblob) (3.8.1)\n",
            "Requirement already satisfied: tqdm in /geode2/home/u010/akvajral/Carbonate/.local/lib/python3.9/site-packages (from nltk>=3.1->textblob) (4.65.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /geode2/home/u010/akvajral/Carbonate/.local/lib/python3.9/site-packages (from nltk>=3.1->textblob) (2023.3.23)\n",
            "Requirement already satisfied: joblib in /geode2/soft/hps/rhel7/python/3.9.8/lib/python3.9/site-packages (from nltk>=3.1->textblob) (1.1.0)\n",
            "Requirement already satisfied: click in /geode2/home/u010/akvajral/Carbonate/.local/lib/python3.9/site-packages (from nltk>=3.1->textblob) (8.1.3)\n",
            "Installing collected packages: textblob\n",
            "Successfully installed textblob-0.17.1\n"
          ]
        }
      ],
      "source": [
        "!pip install textblob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lFUvvwlbHDhY",
        "outputId": "3924f674-7553-4880-e0de-0c4a24123b5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.5103563474387528\n",
            "For Validation:\n",
            "\n",
            "Accuracy: 0.5083518930957683\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.31      0.01      0.02      3499\n",
            "           1       0.51      0.98      0.67      3685\n",
            "\n",
            "    accuracy                           0.51      7184\n",
            "   macro avg       0.41      0.50      0.34      7184\n",
            "weighted avg       0.42      0.51      0.35      7184\n",
            "\n",
            "[[  28 3471]\n",
            " [  61 3624]] \n",
            "\n",
            "\n",
            "For Training:\n",
            "\n",
            "Accuracy: 0.5258926707036959\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.39      0.01      0.02     13542\n",
            "           1       0.53      0.98      0.69     15192\n",
            "\n",
            "    accuracy                           0.53     28734\n",
            "   macro avg       0.46      0.50      0.35     28734\n",
            "weighted avg       0.46      0.53      0.37     28734\n",
            "\n",
            "[[  149 13393]\n",
            " [  230 14962]] \n",
            "\n",
            "\n",
            "For Testing:\n",
            "\n",
            "Accuracy: 0.5103563474387528\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.41      0.01      0.02      4376\n",
            "           1       0.51      0.98      0.67      4604\n",
            "\n",
            "    accuracy                           0.51      8980\n",
            "   macro avg       0.46      0.50      0.35      8980\n",
            "weighted avg       0.46      0.51      0.36      8980\n",
            "\n",
            "[[  49 4327]\n",
            " [  70 4534]] \n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Define a function to calculate sentiment polarity using TextBlob\n",
        "from textblob import TextBlob\n",
        "def get_sentiment(text):\n",
        "    blob = TextBlob(text)\n",
        "    return blob.sentiment.polarity\n",
        "# Perform sentiment analysis and create a new 'sentiment' column\n",
        "df['sentiment'] = df['text'].apply(get_sentiment)\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['sentiment'].values.reshape(-1, 1), df['label'], test_size=0.2, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "# Train a logistic regression model on the training set\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print('Accuracy:', accuracy)\n",
        "\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"For Validation:\\n\")\n",
        "pred_eval(X_val, y_val, model)\n",
        "print(\"For Training:\\n\")\n",
        "pred_eval(X_train, y_train, model)\n",
        "print(\"For Testing:\\n\")\n",
        "pred_eval(X_test, y_test, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f5w4RBo4HDhZ",
        "outputId": "ad3d2d8a-b9bd-4b5d-e749-12a54c6e58b5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0        0.082149\n",
              "1       -0.078947\n",
              "2        0.055931\n",
              "3        0.082727\n",
              "4        0.130252\n",
              "           ...   \n",
              "44893   -0.159386\n",
              "44894    0.072899\n",
              "44895   -0.976562\n",
              "44896    0.053472\n",
              "44897   -0.262879\n",
              "Name: sentiment, Length: 44898, dtype: float64"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['sentiment']"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rAU3QweXHIbS"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}